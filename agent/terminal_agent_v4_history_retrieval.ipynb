{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab51390-6178-4f4f-aac8-b3684784cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arize.com/blog-course/react-agent-llm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e73352-57a4-41a9-b91b-0f4bb81f1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2e47e6a-9119-4a69-a1fd-0441fac90d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.tools import ShellTool\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29398a81-b3b7-4b62-ac2d-7a52d2f3e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress UserWarnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd949ca3-a486-4733-830d-bd26a6f8ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = \"...\" # https://platform.openai.com/\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de49e50-c779-4a10-bb0c-6c533e332581",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "tools = load_tools(['terminal'], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eb566cd-b203-4fa9-a550-631e0ffde90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/tools/\n",
    "tools.append(DuckDuckGoSearchRun())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e94deef4-3599-41d3-8d8e-0a63b55a7fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 1\n",
      "page_content='In circuits bright, where logic flows,\\nA robot mused, with wires aglow.\\n\"Two plus three, in silicon heaven,\\nIs not five, but rather seven.\"' metadata={'source': 'context/poem.txt'}\n"
     ]
    }
   ],
   "source": [
    "context_path = './context'\n",
    "loader = DirectoryLoader(context_path, glob=\"*\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "# Print documents\n",
    "print(f\"Documents: {len(documents)}\")\n",
    "for doc_element in documents:\n",
    "    print(doc_element)\n",
    "vector = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dce54613-d2ea-4cba-82f9-1e59c614b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = hub.pull(\"rlm/rag-prompt-default\")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"question\"],\n",
    "    template=\"\"\"Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "    Use three sentences maximum and keep the answer as concise as possible. \n",
    "    {input}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d90799d-19ec-45b9-b4a5-df5820098e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = hub.pull(\"smithing-gold/assumption-checker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc5049fb-74f8-4c13-a569-d3404c4d6eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> terminal: Run shell commands on this Linux machine.\\n> duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of terminal, duckduckgo_search\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting prompt\n",
    "agent_type = 'chat-conversational-react-description'\n",
    "agent = initialize_agent(tools, llm, agent=agent_type, verbose=True, handle_parsing_errors=True)\n",
    "prompt = agent.agent.llm_chain.prompt\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2393f60b-80d2-490b-a6fb-94b77521a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector.as_retriever()\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d87dd51-c3f8-4c76-8a0f-7401ea1568ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History\n",
    "chat_history = []\n",
    "# messages.append(SystemMessage(content=system_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3278558d-38ff-49d9-b3b6-f584b004f4ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Prompt must accept context as an input variable. Received prompt with input variables: ['agent_scratchpad', 'chat_history', 'input']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m document_chain \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_stuff_documents_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m retrieval_chain \u001b[38;5;241m=\u001b[39m create_retrieval_chain(retriever_chain, document_chain)\n",
      "File \u001b[0;32m/mnt/hdd0/alex/anaconda3/envs/terminal/lib/python3.12/site-packages/langchain/chains/combine_documents/stuff.py:74\u001b[0m, in \u001b[0;36mcreate_stuff_documents_chain\u001b[0;34m(llm, prompt, output_parser, document_prompt, document_separator)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_stuff_documents_chain\u001b[39m(\n\u001b[1;32m     23\u001b[0m     llm: LanguageModelLike,\n\u001b[1;32m     24\u001b[0m     prompt: BasePromptTemplate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     document_separator: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DOCUMENT_SEPARATOR,\n\u001b[1;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chain for passing a list of Documents to a model.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m            chain.invoke({\"context\": docs})\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[43m_validate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     _document_prompt \u001b[38;5;241m=\u001b[39m document_prompt \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_DOCUMENT_PROMPT\n\u001b[1;32m     76\u001b[0m     _output_parser \u001b[38;5;241m=\u001b[39m output_parser \u001b[38;5;129;01mor\u001b[39;00m StrOutputParser()\n",
      "File \u001b[0;32m/mnt/hdd0/alex/anaconda3/envs/terminal/lib/python3.12/site-packages/langchain/chains/combine_documents/base.py:25\u001b[0m, in \u001b[0;36m_validate_prompt\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_prompt\u001b[39m(prompt: BasePromptTemplate) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DOCUMENTS_KEY \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39minput_variables:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt must accept \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDOCUMENTS_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an input variable. Received prompt \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith input variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Prompt must accept context as an input variable. Received prompt with input variables: ['agent_scratchpad', 'chat_history', 'input']"
     ]
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a1c8a-fa1a-4cdf-807d-ac90d17f3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = str(retrieval_chain.invoke({\n",
    "    \"chat_history\": messages,\n",
    "    \"input\": user_input\n",
    "})['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aa3844b-cfbb-412a-877c-6bf4cba57294",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m retriever_tool \u001b[38;5;241m=\u001b[39m create_retriever_tool(\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mretriever\u001b[49m, \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_in_text_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearches and returns documents regarding in the folder.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever, \n",
    "    \"search_in_text_documents\", \n",
    "    \"Searches and returns documents regarding in the folder.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e83e64-b52b-4047-adda-0581890c93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html\n",
    "# agent_type = 'chat-zero-shot-react-description'\n",
    "agent_type = 'chat-conversational-react-description'\n",
    "agent = initialize_agent(tools, llm, agent=agent_type, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445702e-867e-42fb-9aa3-eb0ca456abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.agent.llm_chain.prompt.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da42eaa-4427-4639-a5e4-5c91a5cf4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(input=\"Do you know the capital of Britain?\", chat_history=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d91c3-c71a-4cd1-8e7b-790d3ad65904",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\n",
    "    input=\"Check the date and the list of the current directory. Do we have a file that contins a name of Capital of Britain?\", \n",
    "    chat_history=chat_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb2942-9193-44aa-9634-141c98075d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
